import os
import sys
import zipfile
import shutil
import gdown
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pyspark.sql import SparkSession
from pyspark.ml import PipelineModel
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType

# 1. C·∫•u h√¨nh Java 17 cho Streamlit Cloud
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-17-openjdk-amd64"

# C·∫•u h√¨nh giao di·ªán trang
st.set_page_config(page_title="D·ª± B√°o Doanh Thu E-Commerce", layout="wide")

@st.cache_resource
def init_spark():
    try:
        spark = SparkSession.builder \
            .appName("RevenuePredictor") \
            .master("local[1]") \
            .config("spark.driver.memory", "512m") \
            .config("spark.ui.showConsoleProgress", "false") \
            .getOrCreate()
        return spark
    except Exception as e:
        return None

@st.cache_resource
def download_and_prepare_model():
    final_model_path = "models/random_forest_v1"
    zip_path = "model.zip"
    extract_path = "models/temp_extract"
    
    if os.path.exists(final_model_path):
        return final_model_path
    
    file_id = "1vOwtKC0wc8CoUONJ6Z45wGLnfOkpQBpY"
    try:
        gdown.download(id=file_id, output=zip_path, quiet=False)
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_path)
        
        actual_path = extract_path
        for root, dirs, files in os.walk(extract_path):
            if "metadata" in dirs:
                actual_path = root
                break
        
        os.makedirs("models", exist_ok=True)
        if os.path.exists(final_model_path): shutil.rmtree(final_model_path)
        shutil.move(actual_path, final_model_path)
        
        if os.path.exists(zip_path): os.remove(zip_path)
        if os.path.exists(extract_path): shutil.rmtree(extract_path)
        return final_model_path
    except:
        return None

def main():
    # D√≤ng n√†y b√¢y gi·ªù s·∫Ω kh√¥ng c√≤n l·ªói NameError v√¨ ƒë√£ c√≥ import streamlit as st ·ªü tr√™n
    st.title("üõí H·ªá Th·ªëng D·ª± B√°o Doanh Thu Th∆∞∆°ng M·∫°i ƒêi·ªán T·ª≠")
    st.markdown("---")
    
    spark = init_spark()
    model_path = download_and_prepare_model()
    
    if spark and model_path:
        try:
            model = PipelineModel.load(model_path)
            st.sidebar.success("‚úÖ M√¥ h√¨nh ƒë√£ n·∫°p th√†nh c√¥ng!")
            
            # Sidebar - Th√¥ng tin m√¥ h√¨nh
            st.sidebar.header("Th√¥ng Tin M√¥ H√¨nh")
            st.sidebar.write("**Thu·∫≠t to√°n:** Random Forest Regressor")
            st.sidebar.write("**ƒê·ªô ch√≠nh x√°c (R¬≤):** ~96.6%")
            
            # Giao di·ªán nh·∫≠p li·ªáu
            st.subheader("üìù Nh·∫≠p Th√¥ng Tin S·∫£n Ph·∫©m")
            col1, col2 = st.columns(2)
            with col1:
                category = st.selectbox("Lo·∫°i S·∫£n Ph·∫©m", 
                    ["Electronics", "Clothing", "Books", "Home Appliances", "Toys"])
                region = st.selectbox("Khu V·ª±c (Region)", 
                    ["North America", "Europe", "Asia", "South America", "Oceania"])
                units = st.number_input("S·ªë L∆∞·ª£ng B√°n (Units Sold)", min_value=1, value=150)
            
            with col2:
                discount_app = st.slider("M·ª©c Gi·∫£m Gi√° (Discount_Applied)", 0.0, 1.0, 0.15)
                ad_spend = st.number_input("Chi Ph√≠ Qu·∫£ng C√°o ($)", min_value=0.0, value=120.0)
                clicks = st.number_input("S·ªë L∆∞·ª£t Click (Clicks)", min_value=0, value=25)

            if st.button("üîÆ B·∫Øt ƒê·∫ßu D·ª± B√°o", use_container_width=True):
                # Kh·ªüi t·∫°o d·ªØ li·ªáu ƒë·∫ßu v√†o cho Spark
                schema = StructType([
                    StructField("Category", StringType(), True),
                    StructField("Region", StringType(), True),
                    StructField("Units_Sold", IntegerType(), True),
                    StructField("Discount_Applied", DoubleType(), True),
                    StructField("Ad_Spend", DoubleType(), True),
                    StructField("Clicks", DoubleType(), True)
                ])
                
                input_data = [(str(category), str(region), int(units), float(discount_app), float(ad_spend), float(clicks))]
                df = spark.createDataFrame(input_data, schema)
                
                # D·ª± b√°o
                prediction = model.transform(df).collect()[0]["prediction"]
                
                # Hi·ªÉn th·ªã k·∫øt qu·∫£ Pro
                st.divider()
                st.balloons()
                
                res_col1, res_col2 = st.columns([1, 1])
                with res_col1:
                    st.metric(label="Doanh Thu D·ª± B√°o", value=f"${prediction:,.2f}")
                    profit = prediction - ad_spend
                    st.metric(label="L·ª£i Nhu·∫≠n D·ª± T√≠nh (Sau tr·ª´ QC)", value=f"${profit:,.2f}")
                
                with res_col2:
                    # TR·ª∞C QUAN H√ìA: Feature Importance (D·ª±a tr√™n k·∫øt qu·∫£ t·ª´ Notebook c·ªßa b·∫°n)
                    st.write("### üìä C√°c y·∫øu t·ªë ·∫£nh h∆∞·ªüng nh·∫•t")
                    # D·ªØ li·ªáu m·∫´u d·ª±a tr√™n m√¥ h√¨nh RF c·ªßa b·∫°n
                    importance_data = pd.DataFrame({
                        'Y·∫øu t·ªë': ['S·ªë l∆∞·ª£ng b√°n', 'Ng√†nh h√†ng', 'Qu·∫£ng c√°o', 'Clicks', 'Gi·∫£m gi√°', 'Khu v·ª±c'],
                        'M·ª©c ƒë·ªô (%)': [35, 25, 15, 12, 8, 5]
                    })
                    fig, ax = plt.subplots(figsize=(6, 4))
                    sns.barplot(x='M·ª©c ƒë·ªô (%)', y='Y·∫øu t·ªë', data=importance_data, palette='viridis', ax=ax)
                    st.pyplot(fig)

        except Exception as e:
            st.error(f"L·ªói th·ª±c thi: {e}")

if __name__ == "__main__":
    main()
