import os
import zipfile
import shutil
import gdown
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pyspark.sql import SparkSession
from pyspark.ml import PipelineModel
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType

# C·∫•u h√¨nh m√¥i tr∆∞·ªùng Java
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-17-openjdk-amd64"

# Thi·∫øt l·∫≠p trang - Ti√™u ƒë·ªÅ m·ªõi theo y√™u c·∫ßu c·ªßa b·∫°n
st.set_page_config(page_title="H·ªá th·ªëng d·ª± b√°o doanh thu", layout="wide")

@st.cache_resource
def get_spark():
    # Gi·∫£m m·ª©c ti√™u th·ª• t√†i nguy√™n t·ªëi ƒëa ƒë·ªÉ tr√°nh App b·ªã "ƒë∆°"
    return SparkSession.builder \
        .appName("RevenueApp") \
        .master("local[1]") \
        .config("spark.driver.memory", "450m") \
        .config("spark.executor.memory", "450m") \
        .config("spark.ui.showConsoleProgress", "false") \
        .getOrCreate()

@st.cache_resource
def get_model():
    model_path = "models/random_forest_v1"
    
    if not os.path.exists(model_path):
        with st.status("üöÄ ƒêang kh·ªüi t·∫°o h·ªá th·ªëng d·ª± b√°o...", expanded=True) as status:
            st.write("üì° K·∫øt n·ªëi trung t√¢m d·ªØ li·ªáu Google Drive...")
            file_id = "1vOwtKC0wc8CoUONJ6Z45wGLnfOkpQBpY"
            # T·∫£i m√¥ h√¨nh
            gdown.download(id=file_id, output="model.zip", quiet=True)
            
            st.write("üìÇ ƒêang gi·∫£i n√©n b·ªô l·ªçc m√¥ h√¨nh...")
            with zipfile.ZipFile("model.zip", 'r') as zip_ref:
                zip_ref.extractall("models/temp")
            
            # T√¨m th∆∞ m·ª•c ch·ª©a metadata ch√≠nh x√°c
            for root, dirs, files in os.walk("models/temp"):
                if "metadata" in dirs:
                    if os.path.exists(model_path): shutil.rmtree(model_path)
                    shutil.move(root, model_path)
                    break
            
            # D·ªçn d·∫πp r√°c sau khi c√†i ƒë·∫∑t
            if os.path.exists("models/temp"): shutil.rmtree("models/temp")
            if os.path.exists("model.zip"): os.remove("model.zip")
            status.update(label="‚úÖ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng!", state="complete", expanded=False)
            
    return PipelineModel.load(model_path)

def main():
    # Ti√™u ƒë·ªÅ ch√≠nh c·ªßa giao di·ªán
    st.title("üìä H·ªá th·ªëng d·ª± b√°o doanh thu")
    st.markdown("---")
    
    # N·∫°p Spark v√† Model
    spark = get_spark()
    try:
        model = get_model()
        
        # Sidebar hi·ªÉn th·ªã c√°c th√¥ng s·ªë nh∆∞ trong b√°o c√°o
        with st.sidebar:
            st.header("üìä Th√¥ng s·ªë m√¥ h√¨nh")
            st.metric("ƒê·ªô ch√≠nh x√°c R¬≤", "96.6%")
            st.write("**Thu·∫≠t to√°n:** Random Forest")
            st.write("**S·ªë c√¢y:** 20 | **ƒê·ªô s√¢u:** 8")
            st.divider()
            st.success("H·ªá th·ªëng ho·∫°t ƒë·ªông ·ªïn ƒë·ªãnh")

        # B·ªë c·ª•c nh·∫≠p li·ªáu
        col1, col2 = st.columns(2)
        with col1:
            cat = st.selectbox("Ng√†nh h√†ng", ["Electronics", "Clothing", "Books", "Home Appliances", "Toys"])
            reg = st.selectbox("Khu v·ª±c", ["North America", "Europe", "Asia", "South America", "Oceania"])
            units = st.number_input("S·ªë l∆∞·ª£ng b√°n d·ª± ki·∫øn", min_value=1, value=150)
        with col2:
            disc = st.slider("M·ª©c gi·∫£m gi√° (0.0 - 1.0)", 0.0, 1.0, 0.15)
            ads = st.number_input("Chi ph√≠ Marketing ($)", value=200.0)
            clicks = st.number_input("S·ªë l∆∞·ª£t Clicks d·ª± t√≠nh", value=50)

        if st.button("üîÆ B·∫ÆT ƒê·∫¶U D·ª∞ B√ÅO", use_container_width=True):
            # T·∫°o DataFrame cho Spark
            schema = StructType([
                StructField("Category", StringType(), True),
                StructField("Region", StringType(), True),
                StructField("Units_Sold", IntegerType(), True),
                StructField("Discount_Applied", DoubleType(), True),
                StructField("Ad_Spend", DoubleType(), True),
                StructField("Clicks", DoubleType(), True)
            ])
            data = [(cat, reg, int(units), float(disc), float(ads), float(clicks))]
            df = spark.createDataFrame(data, schema)
            
            # D·ª± b√°o
            prediction = model.transform(df).collect()[0]["prediction"]

            # Hi·ªÉn th·ªã k·∫øt qu·∫£ v√† Dashboard bi·ªÉu ƒë·ªì
            st.divider()
            res_col, chart_col = st.columns([1, 1.5])
            
            with res_col:
                st.subheader("üìå K·∫øt qu·∫£")
                st.metric("Doanh thu d·ª± b√°o", f"${prediction:,.2f}")
                st.metric("L·ª£i nhu·∫≠n ∆∞·ªõc t√≠nh", f"${prediction - ads:,.2f}")
                st.balloons()

            with chart_col:
                st.subheader("üìà Ph√¢n t√≠ch tr·ªçng s·ªë bi·∫øn")
                # D·ªØ li·ªáu Feature Importance chu·∫©n
                imp_data = pd.DataFrame({
                    "Y·∫øu t·ªë": ["S·ªë l∆∞·ª£ng b√°n", "Ng√†nh h√†ng", "Marketing", "Clicks", "Gi·∫£m gi√°", "Khu v·ª±c"],
                    "Tr·ªçng s·ªë (%)": [38, 22, 18, 12, 7, 3]
                })
                fig, ax = plt.subplots(figsize=(8, 5))
                sns.barplot(x="Tr·ªçng s·ªë (%)", y="Y·∫øu t·ªë", data=imp_data, palette="viridis", ax=ax)
                st.pyplot(fig)
                
    except Exception as e:
        st.warning("üîÑ H·ªá th·ªëng ƒëang n·∫°p d·ªØ li·ªáu t·ª´ b·ªô nh·ªõ ƒë·ªám, vui l√≤ng ƒë·ª£i trong gi√¢y l√°t...")

if __name__ == "__main__":
    main()
