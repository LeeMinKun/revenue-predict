import os
import sys
import zipfile
import shutil

# 1. C·∫•u h√¨nh Java 17 cho Streamlit Cloud
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-17-openjdk-amd64"

import streamlit as st
from pyspark.sql import SparkSession
from pyspark.ml import PipelineModel
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType
import gdown

# C·∫•u h√¨nh trang
st.set_page_config(page_title="D·ª± B√°o Doanh Thu", layout="wide")

@st.cache_resource
def init_spark():
    try:
        # C·∫•u h√¨nh t·ªëi gi·∫£n ƒë·ªÉ ti·∫øt ki·ªám RAM tr√™n Cloud
        spark = SparkSession.builder \
            .appName("RevenuePredictor") \
            .master("local[1]") \
            .config("spark.driver.memory", "512m") \
            .config("spark.ui.showConsoleProgress", "false") \
            .getOrCreate()
        return spark
    except Exception as e:
        st.error(f"L·ªói kh·ªüi t·∫°o Spark: {e}")
        return None

@st.cache_resource
def download_and_extract_model():
    model_dir = "models/random_forest_v1"
    zip_path = "model.zip"
    
    if os.path.exists(model_dir):
        return model_dir
    
    # ID file ZIP c·ªßa b·∫°n
    file_id = "1vOwtKC0wc8CoUONJ6Z45wGLnfOkpQBpY"
    
    try:
        # S·ª¨A T·∫†I ƒê√ÇY: D√πng id= thay v√¨ url=
        with st.spinner("üì¶ ƒêang t·∫£i g√≥i m√¥ h√¨nh t·ª´ Google Drive..."):
            # gdown s·∫Ω t·ª± x·ª≠ l√Ω x√°c nh·∫≠n file l·ªõn khi d√πng tham s·ªë id
            gdown.download(id=file_id, output=zip_path, quiet=False)
        
        if os.path.exists(zip_path):
            with st.spinner("üìÇ ƒêang gi·∫£i n√©n..."):
                with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                    zip_ref.extractall("models/")
            os.remove(zip_path)
            return model_dir
        else:
            st.error("Kh√¥ng t√¨m th·∫•y file t·∫£i v·ªÅ.")
            return None
    except Exception as e:
        st.error(f"L·ªói t·∫£i file: {e}")
        return None

def main():
    st.title("üõí H·ªá Th·ªëng D·ª± B√°o Doanh Thu Th∆∞∆°ng M·∫°i ƒêi·ªán T·ª≠")
    
    spark = init_spark()
    model_path = download_and_extract_model()
    
    if spark and model_path:
        try:
            model = PipelineModel.load(model_path)
            st.success("‚úÖ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng v·ªõi m√¥ h√¨nh t·ªëi ∆∞u!")
            
            # Form nh·∫≠p li·ªáu
            col1, col2 = st.columns(2)
            with col1:
                category = st.selectbox("Lo·∫°i S·∫£n Ph·∫©m", ["Electronics", "Home & Kitchen", "Clothing", "Books", "Toys"])
                region = st.selectbox("Khu V·ª±c", ["North", "South", "East", "West"])
                units_sold = st.number_input("S·ªë L∆∞·ª£ng B√°n", min_value=1, value=100)
                discount_applied = st.selectbox("√Åp d·ª•ng gi·∫£m gi√°?", ["Yes", "No"])

            with col2:
                discount_val = st.slider("M·ª©c Gi·∫£m Gi√° (0.0 - 1.0)", 0.0, 1.0, 0.1)
                ad_spend = st.number_input("Chi Ph√≠ Qu·∫£ng C√°o ($)", value=200.0)
                clicks = st.number_input("S·ªë L∆∞·ª£t Click", value=50)
                shipping = st.number_input("Ph√≠ V·∫≠n Chuy·ªÉn ($)", value=5.0)

            if st.button("üîÆ D·ª± B√°o Ngay", use_container_width=True):
                # ƒê·ªãnh nghƒ©a Schema ƒë·∫ßy ƒë·ªß ƒë·ªÉ kh·ªõp v·ªõi Pipeline c≈©
                schema = StructType([
                    StructField("Category", StringType(), True),
                    StructField("Region", StringType(), True),
                    StructField("Discount_Applied", StringType(), True),
                    StructField("Units_Sold", IntegerType(), True),
                    StructField("Discount", DoubleType(), True),
                    StructField("Ad_Spend", DoubleType(), True),
                    StructField("Clicks", DoubleType(), True),
                    StructField("Customer_Reviews", DoubleType(), True), # M·∫∑c ƒë·ªãnh n·∫øu model c·∫ßn
                    StructField("Shipping_Cost", DoubleType(), True)
                ])
                
                input_data = [(str(category), str(region), str(discount_applied), int(units_sold), 
                               float(discount_val), float(ad_spend), float(clicks), 4.0, float(shipping))]
                
                df = spark.createDataFrame(input_data, schema)
                prediction = model.transform(df).collect()[0]["prediction"]
                
                st.divider()
                st.balloons()
                st.header(f"üìä K·∫øt qu·∫£ d·ª± b√°o: ${prediction:,.2f}")
                
        except Exception as e:
            st.error(f"L·ªói th·ª±c thi: {e}")

if __name__ == "__main__":
    main()
