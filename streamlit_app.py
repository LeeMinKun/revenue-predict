import os
import sys
import zipfile
import shutil
import gdown
import streamlit as st
from pyspark.sql import SparkSession
from pyspark.ml import PipelineModel
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType

# C·∫•u h√¨nh Java 17 cho Streamlit Cloud
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-17-openjdk-amd64"

st.set_page_config(page_title="D·ª± B√°o Doanh Thu", layout="wide")

@st.cache_resource
def init_spark():
    try:
        spark = SparkSession.builder \
            .appName("RevenuePredictor") \
            .master("local[1]") \
            .config("spark.driver.memory", "512m") \
            .config("spark.ui.showConsoleProgress", "false") \
            .getOrCreate()
        return spark
    except Exception as e:
        return None

@st.cache_resource
def download_and_prepare_model():
    final_model_path = "models/random_forest_v1"
    zip_path = "model.zip"
    extract_path = "models/temp_extract"
    
    if os.path.exists(final_model_path):
        return final_model_path
    
    file_id = "1vOwtKC0wc8CoUONJ6Z45wGLnfOkpQBpY"
    try:
        gdown.download(id=file_id, output=zip_path, quiet=False)
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_path)
        
        # T√¨m th∆∞ m·ª•c ch·ª©a metadata
        actual_path = extract_path
        for root, dirs, files in os.walk(extract_path):
            if "metadata" in dirs:
                actual_path = root
                break
        
        os.makedirs("models", exist_ok=True)
        if os.path.exists(final_model_path): shutil.rmtree(final_model_path)
        shutil.move(actual_path, final_model_path)
        
        if os.path.exists(zip_path): os.remove(zip_path)
        if os.path.exists(extract_path): shutil.rmtree(extract_path)
        return final_model_path
    except:
        return None

def main():
    st.title("üõí H·ªá Th·ªëng D·ª± B√°o Doanh Thu Th∆∞∆°ng M·∫°i ƒêi·ªán T·ª≠")
    
    spark = init_spark()
    model_path = download_and_prepare_model()
    
    if spark and model_path:
        try:
            model = PipelineModel.load(model_path)
            st.success("‚úÖ M√¥ h√¨nh ƒë√£ s·∫µn s√†ng!")
            
            col1, col2 = st.columns(2)
            with col1:
                category = st.selectbox("Lo·∫°i S·∫£n Ph·∫©m", ["Electronics", "Home & Kitchen", "Clothing", "Books", "Toys"])
                region = st.selectbox("Khu V·ª±c", ["North", "South", "East", "West"])
                units = st.number_input("S·ªë L∆∞·ª£ng B√°n", min_value=1, value=100)
                is_discount = st.radio("C√≥ √°p d·ª•ng gi·∫£m gi√° kh√¥ng?", ["C√≥", "Kh√¥ng"])

            with col2:
                # N·∫øu kh√¥ng gi·∫£m gi√° th√¨ m·ª©c gi·∫£m l√† 0
                discount_slider = st.slider("M·ª©c Gi·∫£m Gi√° (0.01 - 1.0)", 0.01, 1.0, 0.1)
                discount_applied = discount_slider if is_discount == "C√≥" else 0.0
                
                ads = st.number_input("Chi Ph√≠ Qu·∫£ng C√°o ($)", value=200.0)
                clicks = st.number_input("S·ªë L∆∞·ª£t Click (Clicks)", value=50)
                st.info(f"M·ª©c gi·∫£m gi√° √°p d·ª•ng v√†o m√¥ h√¨nh: {discount_applied}")

            if st.button("üîÆ D·ª± B√°o Doanh Thu", use_container_width=True):
                # SCHEMA CHU·∫®N t·ª´ Notebook Nhom1_KPDLL2.ipynb
                # input_cols = ["Units_Sold", "Discount_Applied", "Ad_Spend", "Clicks", "Cat_Vec", "Reg_Vec"]
                schema = StructType([
                    StructField("Category", StringType(), True),
                    StructField("Region", StringType(), True),
                    StructField("Units_Sold", IntegerType(), True),
                    StructField("Discount_Applied", DoubleType(), True), # PH·∫¢I L√Ä DOUBLE
                    StructField("Ad_Spend", DoubleType(), True),
                    StructField("Clicks", DoubleType(), True)
                ])
                
                data = [(str(category), str(region), int(units), float(discount_applied), float(ads), float(clicks))]
                df = spark.createDataFrame(data, schema)
                
                # D·ª± b√°o
                pred_df = model.transform(df)
                result = pred_df.collect()[0]["prediction"]
                
                st.divider()
                st.balloons()
                st.header(f"üìä K·∫øt Qu·∫£ D·ª± B√°o: ${result:,.2f}")
                
        except Exception as e:
            st.error(f"L·ªói load m√¥ h√¨nh: {e}")

if __name__ == "__main__":
    main()
