import os
import sys
import zipfile
import shutil
import gdown
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pyspark.sql import SparkSession
from pyspark.ml import PipelineModel
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType

# C·∫•u h√¨nh Java 17
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-17-openjdk-amd64"

st.set_page_config(page_title="H·ªá th·ªëng D·ª± b√°o Doanh thu", layout="wide")

@st.cache_resource
def init_spark():
    # Th√™m th√¥ng b√°o tr·∫°ng th√°i l√™n UI
    status = st.empty()
    status.info("‚è≥ ƒêang kh·ªüi t·∫°o m√°y ch·ªß t√≠nh to√°n Spark (JVM)...")
    try:
        spark = SparkSession.builder \
            .appName("DSS") \
            .master("local[1]") \
            .config("spark.driver.memory", "400m") \
            .config("spark.executor.memory", "400m") \
            .config("spark.sql.shuffle.partitions", "1") \
            .getOrCreate()
        status.empty()
        return spark
    except Exception as e:
        st.error(f"L·ªói kh·ªüi t·∫°o Spark: {e}")
        return None

@st.cache_resource
def load_model_optimized():
    model_path = "models/random_forest_v1"
    status = st.empty()
    
    if not os.path.exists(model_path):
        status.info("‚è≥ ƒêang k·∫øt n·ªëi Drive v√† t·∫£i m√¥ h√¨nh...")
        file_id = "1vOwtKC0wc8CoUONJ6Z45wGLnfOkpQBpY"
        gdown.download(id=file_id, output="model.zip", quiet=False)
        
        status.info("‚è≥ ƒêang gi·∫£i n√©n g√≥i m√¥ h√¨nh...")
        with zipfile.ZipFile("model.zip", 'r') as zip_ref:
            zip_ref.extractall("models/temp")
        
        for root, dirs, files in os.walk("models/temp"):
            if "metadata" in dirs:
                if os.path.exists(model_path): shutil.rmtree(model_path)
                shutil.move(root, model_path)
                break
        shutil.rmtree("models/temp")
        if os.path.exists("model.zip"): os.remove("model.zip")
    
    status.info("‚è≥ ƒêang n·∫°p m√¥ h√¨nh v√†o Spark (b∆∞·ªõc n√†y c√≥ th·ªÉ m·∫•t 1-2 ph√∫t)...")
    model = PipelineModel.load(model_path)
    status.empty()
    return model

def main():
    st.title("üõí H·ªá Th·ªëng H·ªó Tr·ª£ Ra Quy·∫øt ƒê·ªãnh Doanh Thu")
    
    # Kh·ªüi t·∫°o c√°c th√†nh ph·∫ßn n·∫∑ng
    spark = init_spark()
    if spark:
        try:
            model = load_model_optimized()
            st.sidebar.success("‚úÖ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng!")
            
            # --- DASHBOARD SIDEBAR ---
            st.sidebar.header("üìä Th√¥ng s·ªë m√¥ h√¨nh")
            st.sidebar.write("- **R¬≤ Score:** 96.6%")
            st.sidebar.write("- **Thu·∫≠t to√°n:** Random Forest")

            # --- GIAO DI·ªÜN NH·∫¨P LI·ªÜU ---
            col1, col2 = st.columns(2)
            with col1:
                cat = st.selectbox("Ng√†nh h√†ng", ["Electronics", "Clothing", "Books", "Home Appliances", "Toys"])
                reg = st.selectbox("Khu v·ª±c", ["North America", "Europe", "Asia", "South America", "Oceania"])
                units = st.number_input("S·ªë l∆∞·ª£ng b√°n", min_value=1, value=150)
            with col2:
                disc = st.slider("M·ª©c gi·∫£m gi√°", 0.0, 1.0, 0.15)
                ads = st.number_input("Ng√¢n s√°ch Marketing ($)", value=200.0)
                clicks = st.number_input("S·ªë l∆∞·ª£t Clicks", value=50)

            if st.button("üîÆ TH·ª∞C HI·ªÜN D·ª∞ B√ÅO", use_container_width=True):
                schema = StructType([
                    StructField("Category", StringType(), True),
                    StructField("Region", StringType(), True),
                    StructField("Units_Sold", IntegerType(), True),
                    StructField("Discount_Applied", DoubleType(), True),
                    StructField("Ad_Spend", DoubleType(), True),
                    StructField("Clicks", DoubleType(), True)
                ])
                data = [(cat, reg, int(units), float(disc), float(ads), float(clicks))]
                df = spark.createDataFrame(data, schema)
                
                prediction = model.transform(df).collect()[0]["prediction"]
                
                st.divider()
                r_col, c_col = st.columns([1, 1.5])
                with r_col:
                    st.metric("Doanh thu d·ª± b√°o", f"${prediction:,.2f}")
                    st.metric("L·ª£i nhu·∫≠n d·ª± t√≠nh", f"${prediction - ads:,.2f}")
                
                with c_col:
                    # V·∫Ω bi·ªÉu ƒë·ªì m·ª©c ƒë·ªô ·∫£nh h∆∞·ªüng (Feature Importance)
                    importances = [0.35, 0.25, 0.15, 0.12, 0.08, 0.05] # D·ªØ li·ªáu t·ª´ notebook
                    labels = ["Units Sold", "Category", "Ad Spend", "Clicks", "Discount", "Region"]
                    fig, ax = plt.subplots()
                    sns.barplot(x=importances, y=labels, palette="viridis", ax=ax)
                    st.pyplot(fig)

        except Exception as e:
            st.error(f"L·ªói n·∫°p h·ªá th·ªëng: {e}")

if __name__ == "__main__":
    main()
