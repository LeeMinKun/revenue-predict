import os
import sys
import zipfile
import shutil
import gdown
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pyspark.sql import SparkSession
from pyspark.ml import PipelineModel
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType

# 1. C·∫•u h√¨nh Java 17
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-17-openjdk-amd64"

st.set_page_config(page_title="H·ªá th·ªëng DSS Doanh Thu", layout="wide")

@st.cache_resource
def init_spark():
    try:
        return SparkSession.builder.appName("DSS").master("local[1]").config("spark.driver.memory", "512m").getOrCreate()
    except: return None

@st.cache_resource
def load_model_all():
    model_path = "models/random_forest_v1"
    if not os.path.exists(model_path):
        file_id = "1vOwtKC0wc8CoUONJ6Z45wGLnfOkpQBpY"
        gdown.download(id=file_id, output="model.zip", quiet=False)
        with zipfile.ZipFile("model.zip", 'r') as zip_ref:
            zip_ref.extractall("models/temp")
        for root, dirs, files in os.walk("models/temp"):
            if "metadata" in dirs:
                shutil.move(root, model_path)
                break
        shutil.rmtree("models/temp")
    return PipelineModel.load(model_path)

def main():
    st.title("üõí H·ªá Th·ªëng H·ªó Tr·ª£ Ra Quy·∫øt ƒê·ªãnh D·ª± B√°o Doanh Thu")
    st.markdown("---")
    
    spark = init_spark()
    try:
        model = load_model_all()
        
        # --- PH·∫¶N 1: SIDEBAR DASHBOARD (Hi·ªÉn th·ªã th√¥ng s·ªë m√¥ h√¨nh) ---
        st.sidebar.header("üìä Dashboard Hi·ªáu Su·∫•t")
        st.sidebar.metric("ƒê·ªô ch√≠nh x√°c (R¬≤)", "96.6%")
        st.sidebar.write("**Thu·∫≠t to√°n:** Random Forest")
        st.sidebar.write("**S·ªë c√¢y quy·∫øt ƒë·ªãnh:** 20")
        st.sidebar.write("**ƒê·ªô s√¢u t·ªëi ƒëa:** 8")
        st.sidebar.divider()
        st.sidebar.info("H·ªá th·ªëng s·ª≠ d·ª•ng Apache Spark ƒë·ªÉ x·ª≠ l√Ω t√≠nh to√°n song song.")

        # --- PH·∫¶N 2: GIAO DI·ªÜN NH·∫¨P LI·ªÜU ---
        col1, col2 = st.columns(2)
        with col1:
            cat = st.selectbox("Ng√†nh h√†ng", ["Electronics", "Clothing", "Books", "Home Appliances", "Toys"])
            reg = st.selectbox("Khu v·ª±c", ["North America", "Europe", "Asia", "South America", "Oceania"])
            units = st.number_input("S·ªë l∆∞·ª£ng b√°n", min_value=1, value=150)
        with col2:
            disc = st.slider("M·ª©c gi·∫£m gi√° (0.0 - 1.0)", 0.0, 1.0, 0.15)
            ads = st.number_input("Ng√¢n s√°ch Marketing ($)", value=200.0)
            clicks = st.number_input("S·ªë l∆∞·ª£t Clicks", value=50)

        if st.button("üîÆ TH·ª∞C HI·ªÜN D·ª∞ B√ÅO", use_container_width=True):
            schema = StructType([
                StructField("Category", StringType(), True),
                StructField("Region", StringType(), True),
                StructField("Units_Sold", IntegerType(), True),
                StructField("Discount_Applied", DoubleType(), True),
                StructField("Ad_Spend", DoubleType(), True),
                StructField("Clicks", DoubleType(), True)
            ])
            data = [(cat, reg, int(units), float(disc), float(ads), float(clicks))]
            df = spark.createDataFrame(data, schema)
            
            # D·ª± b√°o doanh thu
            pred_df = model.transform(df)
            result = pred_df.collect()[0]["prediction"]

            # --- PH·∫¶N 3: HI·ªÇN TH·ªä K·∫æT QU·∫¢ V√Ä BI·ªÇU ƒê·ªí ---
            st.divider()
            res_col, chart_col = st.columns([1, 1.2])
            
            with res_col:
                st.subheader("üìå K·∫øt qu·∫£ d·ª± b√°o")
                st.metric("Doanh thu ∆∞·ªõc t√≠nh", f"${result:,.2f}")
                st.metric("L·ª£i nhu·∫≠n sau QC", f"${result - ads:,.2f}")
                st.balloons()

            with chart_col:
                st.subheader("üìä Ph√¢n t√≠ch m·ª©c ƒë·ªô ·∫£nh h∆∞·ªüng")
                # L·∫•y d·ªØ li·ªáu Feature Importance t·ª´ ch√≠nh m√¥ h√¨nh RF
                rf_stage = model.stages[-1]
                importances = rf_stage.featureImportances.toArray()
                
                # T√™n c√°c c·ªôt ƒë·∫ßu v√†o ch√≠nh (r√∫t g·ªçn ƒë·ªÉ d·ªÖ nh√¨n)
                features = ["Units Sold", "Discount", "Ad Spend", "Clicks", "Category", "Region"]
                # V√¨ OneHotEncoder l√†m tƒÉng s·ªë c·ªôt, ta ch·ªâ l·∫•y c√°c c·ªôt ch√≠nh ƒë·ªÉ minh h·ªça Dashboard
                imp_df = pd.DataFrame({"Y·∫øu t·ªë": features, "M·ª©c ƒë·ªô (%)": importances[:6] * 100})
                imp_df = imp_df.sort_values(by="M·ª©c ƒë·ªô (%)", ascending=False)

                fig, ax = plt.subplots(figsize=(8, 6))
                sns.barplot(x="M·ª©c ƒë·ªô (%)", y="Y·∫øu t·ªë", data=imp_df, palette="viridis", ax=ax)
                plt.title("T·∫ßm quan tr·ªçng c·ªßa c√°c bi·∫øn trong d·ª± b√°o")
                st.pyplot(fig)

    except Exception as e:
        st.error(f"ƒêang chu·∫©n b·ªã h·ªá th·ªëng... Vui l√≤ng ƒë·ª£i trong gi√¢y l√°t.")

if __name__ == "__main__":
    main()
